{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2eZzUN2Q2HE5Q8NutrMIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niktej8/PRACTICE/blob/main/PySPARK_EVERYTHING_TO_PRACTICE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYZBXhgiaEns",
        "outputId": "2b088c3b-637e-4c03-86d3-4bb050724a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxt-dev libxtst6 libxxf86dga1\n",
            "  openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
            "  session-migration x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxt-dev libxtst6 libxxf86dga1\n",
            "  openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
            "  openjdk-11-jre-headless session-migration x11-utils\n",
            "0 upgraded, 20 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 122 MB of archives.\n",
            "After this operation, 275 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Ign:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Ign:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Ign:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Ign:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "Err:17 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 185.125.190.39 80]\n",
            "Err:18 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 185.125.190.39 80]\n",
            "Err:19 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 185.125.190.39 80]\n",
            "Err:20 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1\n",
            "  404  Not Found [IP: 185.125.190.39 80]\n",
            "Fetched 4,117 kB in 3s (1,553 kB/s)\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jre-headless_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jre_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk-headless_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 185.125.190.39 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcgJzSfbkV4-"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"RDD'S\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "KoUdqj2NkfQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelize - Convert list to RDDS"
      ],
      "metadata": {
        "id": "eA0pgvVApSgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])"
      ],
      "metadata": {
        "id": "F5L8ViFhklqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect()"
      ],
      "metadata": {
        "id": "o0UbRvwjpWUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQYYcmjpnwIh",
        "outputId": "9182af37-4c01-462b-ded1-6febc0c09855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count()"
      ],
      "metadata": {
        "id": "h41s4YaWpY5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsH6H6Gjnxuo",
        "outputId": "899704f6-c2b6-49e1-867f-6ca79a0ad28c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter()"
      ],
      "metadata": {
        "id": "_21N8pmJpbt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.filter(lambda x : x%2==0).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiAB9VaFkpj5",
        "outputId": "5cc5e10a-1037-427a-dc52-64798a2f0873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map()"
      ],
      "metadata": {
        "id": "AlAK_ohvpfi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.map(lambda x : x**2).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5TO58BBkvjs",
        "outputId": "b69a39e5-a00e-49ed-dee3-697ff5ce1bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce()"
      ],
      "metadata": {
        "id": "ywHuBZ_xphU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.reduce(lambda x,y:x+y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Cd8itEk7eB",
        "outputId": "9461459e-0a2d-42db-a2ae-ea0718fecf46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## countbyvalue()\n",
        "✅ Use it when:\n",
        "You are working with an RDD of single elements (not key-value), like just strings or numbers.\n",
        "You want to count how many times each unique value appears."
      ],
      "metadata": {
        "id": "fyUxLm9uplcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd9DAgjCn0-5",
        "outputId": "a3040e36-1413-492d-f492-b9c245a1b3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## countByKey()\n",
        "✅ Use it when:\n",
        "You are working with an RDD of key-value pairs (i.e., an RDD like (\"user1\", \"page1\"), etc.)\n",
        "You want to count how many times each key appears."
      ],
      "metadata": {
        "id": "J_-s_D9kpqM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a.map(lambda x:(x,1)).collect()\n",
        "a.map(lambda x:(x,1)).countByKey()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScTK7Co6n5x5",
        "outputId": "095b84c4-f694-44f6-c80c-7a140df3f542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  reduceByKey()\n",
        "✅ Use it when:\n",
        "You have an RDD of (key, value) pairs, and you want to reduce values for each key separately."
      ],
      "metadata": {
        "id": "I7H1_ksfqHg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4)])\n",
        "result = rdd.reduceByKey(lambda a, b: a + b).collect()\n",
        "print(result)  # Output: [('a', 4), ('b', 6)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jqmUOlKo27X",
        "outputId": "9fefbc3c-1353-4d8a-a102-80c5953a4807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('b', 6), ('a', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# text_rdd = spark.sparkContext.textFile(\"pk.txt\")"
      ],
      "metadata": {
        "id": "EOgAhKEHk-_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-iWT3521qfs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd.flatMap(lambda x : x.split(' ')).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "collapsed": true,
        "id": "6UEJBR8PlaWb",
        "outputId": "c4054548-7f73-4c52-eb30-ff769d83027b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/pk.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/pk.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 34 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2882974319.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/pk.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:57)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/pk.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 34 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mapValues()\n",
        "✅ Use it when:\n",
        "You have an RDD of (key, value) pairs, and you want to transform only the value, leaving the key unchanged."
      ],
      "metadata": {
        "id": "e1uBRslfrXQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2)])\n",
        "rdd.mapValues(lambda x:x*100).collect()\n"
      ],
      "metadata": {
        "id": "C7UzNv0nmJFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## flatMapValues()\n",
        "✅ Use it when:\n",
        "You have an RDD of (key, value) pairs, and each value is a collection (like a list or string), and you want to keep the key but \"explode\" the value into multiple items.\n",
        "\n",
        "It’s like mapValues() + flattening."
      ],
      "metadata": {
        "id": "X6h8Vgk5r7Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(\"user1\", \"apple orange banana\"), (\"user2\", \"grapes banana\")])\n",
        "rdd.flatMapValues(lambda x:x.split(\" \")).collect()"
      ],
      "metadata": {
        "id": "4KebWWBpnW-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distinct()"
      ],
      "metadata": {
        "id": "mCmpr5KItn2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = spark.sparkContext.parallelize([1,1,2,3,3,3,2,4,5])\n",
        "b.distinct().collect()"
      ],
      "metadata": {
        "id": "aD9Y3UPuoKOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## union"
      ],
      "metadata": {
        "id": "HcacGSutt4cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = spark.sparkContext.parallelize([6,7,8,2,3,10])\n",
        "b.union(c).collect()"
      ],
      "metadata": {
        "id": "6KrcG-3nttmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## intersection(otherRDD)\n",
        "Returns only elements that are common to both RDDs."
      ],
      "metadata": {
        "id": "r8l6SPIKuGVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b.intersection(c).collect()"
      ],
      "metadata": {
        "id": "FWgMLG8Tt8wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## subtract(otherRDD)\n",
        "Returns elements from the source RDD that are not in the second RDD."
      ],
      "metadata": {
        "id": "RQiSY8-0uQiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b.subtract(c).collect()"
      ],
      "metadata": {
        "id": "MEfGCWXXuMgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cartesian(otherRDD)\n",
        "Returns the Cartesian product (like nested loops)."
      ],
      "metadata": {
        "id": "dO8rkhSiucHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartesian = b.cartesian(c)"
      ],
      "metadata": {
        "id": "q_EAz1A5uVOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DkxLYEhKuqLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check how many partitions data is divided"
      ],
      "metadata": {
        "id": "B8Dm7HuEuwzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartesian.getNumPartitions()"
      ],
      "metadata": {
        "id": "I3wOqOUauiK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## repartition"
      ],
      "metadata": {
        "id": "rTXzPV2jvhv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartesian = cartesian.repartition(10)\n",
        "cartesian.getNumPartitions()"
      ],
      "metadata": {
        "id": "R0_Eh9buu0Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only decrease the partitions not increase"
      ],
      "metadata": {
        "id": "T5rTuuzjveLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartesian = cartesian.coalesce(11)\n",
        "cartesian.getNumPartitions()"
      ],
      "metadata": {
        "id": "2pYTAudyvCX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## we can also ZIP the values"
      ],
      "metadata": {
        "id": "3_VlZzjtwz9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "y = spark.sparkContext.parallelize([6,7,8,9,10])\n",
        "x.zip(y).collect()"
      ],
      "metadata": {
        "id": "58gKXEabwkWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GKGJ5MDuxBf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ZIP with Index"
      ],
      "metadata": {
        "id": "ThxDfoeQxBZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.zipWithIndex().collect()"
      ],
      "metadata": {
        "id": "SfjqDW-Zw5_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## groupBy(func)\n",
        "What it does: Groups elements in an RDD based on the result of a function you provide. Returns pairs (key, iterable_of_values) where key is the result of your function."
      ],
      "metadata": {
        "id": "8sEmV3BayK-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6])\n",
        "rdd.groupBy(lambda x : \"even\" if x%2==0 else \"odd\").mapValues(lambda x:list(x)).collect()"
      ],
      "metadata": {
        "id": "u_HP55g1x5Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## groupBykey\n",
        "What it does: Used on pair RDDs (key-value pairs). Groups all values that share the same key."
      ],
      "metadata": {
        "id": "n0Pn9xqLyiHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([('a', 1), ('b', 2), ('a', 3), ('b', 4), ('c', 5)])\n",
        "rdd.groupByKey().mapValues(lambda x:list(x)).collect()"
      ],
      "metadata": {
        "id": "2q3OY2L0yP-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sort by"
      ],
      "metadata": {
        "id": "jYWVgj_fzBdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6])\n",
        "rdd.sortBy(lambda x:x,ascending=False).collect()"
      ],
      "metadata": {
        "id": "eYXb5bVwyoN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sortby key"
      ],
      "metadata": {
        "id": "fSF3pK8bzKe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([(3, \"c\"), (1, \"a\"), (2, \"b\")])\n",
        "rdd = rdd.sortByKey()"
      ],
      "metadata": {
        "id": "CIoO-tNyzFHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(2)"
      ],
      "metadata": {
        "id": "PZGlaWhWzf1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takesample\n",
        "\n",
        "replacement either True/False,number, seed"
      ],
      "metadata": {
        "id": "1b1mLCrm0hXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.takeSample(False,10)"
      ],
      "metadata": {
        "id": "p50aXMIwzhJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.takeSample(True,5,seed=8)"
      ],
      "metadata": {
        "id": "6_HerBJvzs0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.takeSample(True,5,seed=8)"
      ],
      "metadata": {
        "id": "VHm1MrWP0Vyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeordered will get smallest number of elements"
      ],
      "metadata": {
        "id": "roSZD_zG02Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6])\n",
        "rdd.takeOrdered(2)"
      ],
      "metadata": {
        "id": "SDJ4b7VX0eBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top n largest elements"
      ],
      "metadata": {
        "id": "VpDqP3rr08Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.top(2)"
      ],
      "metadata": {
        "id": "i1sLs1Wy00lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \tReturns the first element."
      ],
      "metadata": {
        "id": "aXosKxD51D2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWMtqT7NvoBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.first()"
      ],
      "metadata": {
        "id": "Gl66qik-07pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie Ratings Analytics"
      ],
      "metadata": {
        "id": "7N6_iRP01LgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "movies.txt\n",
        "movie_id,title,genre\n",
        "1,Titanic,Romance\n",
        "2,Avatar,Sci-Fi\n",
        "3,The Dark Knight,Action\n",
        "4,Inception,Sci-Fi\n",
        "5,The Godfather,Crime\n",
        "\n",
        "----------------------------------\n",
        "user_id,movie_id,rating\n",
        "101,1,5\n",
        "102,2,4\n",
        "101,3,5\n",
        "103,1,4\n",
        "104,2,3\n",
        "101,4,4\n",
        "102,5,5\n",
        "105,4,5\n",
        "106,3,3"
      ],
      "metadata": {
        "id": "Z5T8jVXN1Oq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read both files using textFile() and parse each line (skip headers)."
      ],
      "metadata": {
        "id": "2S1kPFxc4j1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies = spark.sparkContext.textFile(\"movies.txt\")\n",
        "ratings = spark.sparkContext.textFile(\"ratings.txt\")\n",
        "header1 = movies.first()\n",
        "header2 = ratings.first()\n",
        "movies = movies.filter(lambda x : x != header1)\n",
        "ratings = ratings.filter(lambda x : x != header2)"
      ],
      "metadata": {
        "id": "M3vmwSho1Arm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_j = ratings.map(lambda x : (x.split(',')[1],\n",
        " (x.split(',')[0],x.split(',')[2])))\n",
        "movies_j = movies.map(lambda x : (x.split(',')[0],\n",
        " (x.split(',')[1],x.split(',')[2])))"
      ],
      "metadata": {
        "id": "Mgo4RB384823"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_joined = movies_j.leftOuterJoin(ratings_j)"
      ],
      "metadata": {
        "id": "UTZUzndY8yFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_joined.map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1][0],x[1][1][1])))"
      ],
      "metadata": {
        "id": "h7a3DJ8HDJUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_rating = movies_joined.map(lambda x: (x[1][0][0],int(x[1][1][1])))\n",
        "movie_rating.groupByKey().mapValues(lambda x : sum(x)/len(x)).sortBy(lambda x :x[1],ascending = False).take(3)"
      ],
      "metadata": {
        "id": "HH5MKtw3E-yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_joined.map(lambda x : (x[1][0][1],1)).reduceByKey(lambda x,y : x+y).sortBy(lambda x :x[1],ascending = False).collect()"
      ],
      "metadata": {
        "id": "COZQyIG4HJLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.collect()"
      ],
      "metadata": {
        "id": "bqFdI36aFysM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.map(lambda x : (x.split(\",\")[0],1)).groupByKey().mapValues(lambda x:sum(x)).filter(lambda x : x[1]>2).collect()"
      ],
      "metadata": {
        "id": "RG-VXX3tHCNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies.map(lambda x : x.split(\",\")[2]).distinct().collect()"
      ],
      "metadata": {
        "id": "e5xOstuzF3bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies.zipWithIndex().collect()"
      ],
      "metadata": {
        "id": "Xn4TIc55GV-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_rating.groupByKey().mapValues(lambda x : x).collect()"
      ],
      "metadata": {
        "id": "cPv1yeOvFJiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_joined.mapValues(lambda x : x.split(',')).collect"
      ],
      "metadata": {
        "id": "ljH7U0FBDPMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1 = ratings.map(lambda x:(int(x.split(\",\")[1]),(x.split(',')[0],x.split(\",\")[2])))"
      ],
      "metadata": {
        "id": "fgSxdFJV5V3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1.collect()"
      ],
      "metadata": {
        "id": "2GAdVMje3Z-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1.collect()"
      ],
      "metadata": {
        "id": "LJan29dV1oqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined  = r1.join(m1)"
      ],
      "metadata": {
        "id": "7A4JI6Pq21nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined.collect()"
      ],
      "metadata": {
        "id": "f1biCGYy7-lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a pyspark dataframe by using list of tuples"
      ],
      "metadata": {
        "id": "VKIMd9Xy35XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "cMAexVZw8Bro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
      ],
      "metadata": {
        "id": "8rQTYmWQ4QPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('Nikhil',25),(\"Raghava\",26),('Surya',23)]\n",
        "columns = ['Name','Age']\n",
        "\n",
        "df = spark.createDataFrame(data,columns)"
      ],
      "metadata": {
        "id": "_Wc-gtrd4Eai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of Dictionaries"
      ],
      "metadata": {
        "id": "Shj_e7VZ41gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{\"name\":\"Nikhil\",\"Age\":25},{\"name\":\"Raghava\",'Age':23}]\n",
        "\n",
        "df = spark.createDataFrame(data)"
      ],
      "metadata": {
        "id": "-w4lwXWs4w2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "ziIR7tIv4yvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1,'a'),(2,'b'),(3,'c')]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "i8Ha_eCA5ONY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df =pd.read_excel(\"DAILY_NOTES.xlsx\")"
      ],
      "metadata": {
        "id": "a0AGyMnA5bF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.createDataFrame(df).show()"
      ],
      "metadata": {
        "id": "Kr5RBoXZ5cLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = spark.read.format('parquet').load('titanic.parquet')"
      ],
      "metadata": {
        "id": "AeVmVPTl7F37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)])\n",
        "spark.createDataFrame([('Alice', 1)], schema).show()\n"
      ],
      "metadata": {
        "id": "iRDNA81x7XVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPgO2mpd8K_N",
        "outputId": "e9036c50-22f1-48db-e680-5cd867ef23d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PassengerId',\n",
              " 'Survived',\n",
              " 'Pclass',\n",
              " 'Name',\n",
              " 'Sex',\n",
              " 'Age',\n",
              " 'SibSp',\n",
              " 'Parch',\n",
              " 'Ticket',\n",
              " 'Fare',\n",
              " 'Cabin',\n",
              " 'Embarked']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## show will return top 20 rows"
      ],
      "metadata": {
        "id": "nNHxiLne8vh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqVhVUz8c9L",
        "outputId": "1c5f1c1d-9ed9-4377-ab46-c688648f50ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.take(3) # first 3 rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6qIji98hVX",
        "outputId": "0f5d3685-a766-462e-f3d8-d20a4d2cb3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S'),\n",
              " Row(PassengerId=2, Survived=1, Pclass=1, Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex='female', Age=38.0, SibSp=1, Parch=0, Ticket='PC 17599', Fare=71.2833, Cabin='C85', Embarked='C'),\n",
              " Row(PassengerId=3, Survived=1, Pclass=3, Name='Heikkinen, Miss. Laina', Sex='female', Age=26.0, SibSp=0, Parch=0, Ticket='STON/O2. 3101282', Fare=7.925, Cabin=None, Embarked='S')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.tail(2) #last 2 rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi-WSh1v80VS",
        "outputId": "ed50cbbe-753e-442b-aa63-917e77a2d0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(PassengerId=890, Survived=1, Pclass=1, Name='Behr, Mr. Karl Howell', Sex='male', Age=26.0, SibSp=0, Parch=0, Ticket='111369', Fare=30.0, Cabin='C148', Embarked='C'),\n",
              " Row(PassengerId=891, Survived=0, Pclass=3, Name='Dooley, Mr. Patrick', Sex='male', Age=32.0, SibSp=0, Parch=0, Ticket='370376', Fare=7.75, Cabin=None, Embarked='Q')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.head()  # first row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n57bTNE-87mm",
        "outputId": "c09cfcff-8d33-41be-873d-0b5f024f27bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund, Mr. Owen Harris', Sex='male', Age=22.0, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.columns # for columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg2tfbaa9FB6",
        "outputId": "57bda83e-c397-42e0-a304-6c433ccd6f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PassengerId',\n",
              " 'Survived',\n",
              " 'Pclass',\n",
              " 'Name',\n",
              " 'Sex',\n",
              " 'Age',\n",
              " 'SibSp',\n",
              " 'Parch',\n",
              " 'Ticket',\n",
              " 'Fare',\n",
              " 'Cabin',\n",
              " 'Embarked']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic.count()) # rows\n",
        "len(titanic.columns) # columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRpnycnC9KQS",
        "outputId": "42fe6083-7298-43f3-b394-fb35f8cb79c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.dtypes #datatypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbrVf_lB9H4u",
        "outputId": "1ee7a21f-d042-45e3-d463-5e7d9d9252b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('PassengerId', 'bigint'),\n",
              " ('Survived', 'bigint'),\n",
              " ('Pclass', 'bigint'),\n",
              " ('Name', 'string'),\n",
              " ('Sex', 'string'),\n",
              " ('Age', 'double'),\n",
              " ('SibSp', 'bigint'),\n",
              " ('Parch', 'bigint'),\n",
              " ('Ticket', 'string'),\n",
              " ('Fare', 'double'),\n",
              " ('Cabin', 'string'),\n",
              " ('Embarked', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.schema # entire schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2uVWTBz9WMy",
        "outputId": "f87574aa-1003-4844-8338-382661ee62e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('PassengerId', LongType(), True), StructField('Survived', LongType(), True), StructField('Pclass', LongType(), True), StructField('Name', StringType(), True), StructField('Sex', StringType(), True), StructField('Age', DoubleType(), True), StructField('SibSp', LongType(), True), StructField('Parch', LongType(), True), StructField('Ticket', StringType(), True), StructField('Fare', DoubleType(), True), StructField('Cabin', StringType(), True), StructField('Embarked', StringType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07LjUHPF9d4G",
        "outputId": "793cc53a-c19e-4e25-dc1d-15fa85ec1922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PassengerId: long (nullable = true)\n",
            " |-- Survived: long (nullable = true)\n",
            " |-- Pclass: long (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: long (nullable = true)\n",
            " |-- Parch: long (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.select('Sex','Fare','Cabin').distinct().show() # select distinct sex,fare,cabin columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lTSQG_M9iRz",
        "outputId": "5b417a3f-0f0d-4656-89d6-ac547488749c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----------+\n",
            "|   Sex|    Fare|      Cabin|\n",
            "+------+--------+-----------+\n",
            "|female|   263.0|C23 C25 C27|\n",
            "|  male|    79.2|        B86|\n",
            "|female| 83.1583|        C54|\n",
            "|  male|    30.5|        C30|\n",
            "|female|    86.5|        B77|\n",
            "|female|    57.0|        B20|\n",
            "|  male|  151.55|    C22 C26|\n",
            "|female|    90.0|        C93|\n",
            "|female|    71.0|        B22|\n",
            "|female|   120.0|    B96 B98|\n",
            "|female|    39.4|        D28|\n",
            "|  male|247.5208|    B58 B60|\n",
            "|  male|   120.0|    B96 B98|\n",
            "|  male| 81.8583|        A34|\n",
            "|female| 77.9583|         D9|\n",
            "|female|    13.0|        F33|\n",
            "|  male| 52.5542|        D19|\n",
            "|  male|    30.5|       C106|\n",
            "|  male|    57.0|        B20|\n",
            "|female| 76.2917|        D15|\n",
            "+------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,lit,asc,desc,round"
      ],
      "metadata": {
        "id": "ktZDSS6W-JUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwj0TkQnTizD",
        "outputId": "6dc0aac2-df24-47ff-d7b0-756a30143c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add a column and Modify column"
      ],
      "metadata": {
        "id": "1Qd4KQ0SUDvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.withColumn('Fare',round(col('Fare'))).show(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGsWPGGiTp_a",
        "outputId": "0d84ed24-5d25-4de7-9321-f64e326e4805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+---------+----+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+---------+----+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171| 7.0| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.0|  C85|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+---------+----+-----+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_d = titanic.withColumn('Nikhil',col('Fare')*10)"
      ],
      "metadata": {
        "id": "sjsXGihc9l0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drop a column and multiple columns"
      ],
      "metadata": {
        "id": "qM4Yh32qUM5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.drop(col('Sex'),col('Fare')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7-G6w---R-z",
        "outputId": "5afe9702-3759-4eec-dfcf-e7b0cbbd603c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+----+-----+-----+----------------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name| Age|SibSp|Parch|          Ticket|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+----+-----+-----+----------------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|22.0|    1|    0|       A/5 21171| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|38.0|    1|    0|        PC 17599|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|26.0|    0|    0|STON/O2. 3101282| NULL|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|35.0|    1|    0|          113803| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|35.0|    0|    0|          373450| NULL|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|NULL|    0|    0|          330877| NULL|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|54.0|    0|    0|           17463|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...| 2.0|    3|    1|          349909| NULL|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|27.0|    0|    2|          347742| NULL|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|14.0|    1|    0|          237736| NULL|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...| 4.0|    1|    1|         PP 9549|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|58.0|    0|    0|          113783| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|20.0|    0|    0|       A/5. 2151| NULL|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|39.0|    1|    5|          347082| NULL|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|14.0|    0|    0|          350406| NULL|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|55.0|    0|    0|          248706| NULL|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene| 2.0|    4|    1|          382652| NULL|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|NULL|    0|    0|          244373| NULL|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|31.0|    1|    0|          345763| NULL|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|NULL|    0|    0|            2649| NULL|       C|\n",
            "+-----------+--------+------+--------------------+----+-----+-----+----------------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename a columns"
      ],
      "metadata": {
        "id": "5aV9CS9mUQ54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.withColumnRenamed('Fare','Updated_Fare').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOZm1JOz-dYP",
        "outputId": "ce83738b-daa2-4494-d25a-1ef5fb5da815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|Updated_Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|        7.25| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|     71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|       7.925| NULL|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|        53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|        8.05| NULL|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+------------+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Multiple columns"
      ],
      "metadata": {
        "id": "Kjktz0xmUWCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.withColumns({'fare2':lit(20),'fare3':lit(50)}).show()"
      ],
      "metadata": {
        "id": "7Ni0l3aH-3aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alias"
      ],
      "metadata": {
        "id": "gc0wedCdUYz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.alias('t').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC-Sbpk2_ENE",
        "outputId": "221ad80b-1d23-4d3a-867f-2d393f502212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conditions"
      ],
      "metadata": {
        "id": "hit_6CBCUemY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.where(~ (col('Sex')=='male') & (col('Age')>30)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-USdUqtH_gd7",
        "outputId": "84ab95d5-5ad8-40b0-dc3e-19390dd009cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+------------+--------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|      Ticket|    Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+------------+--------+-----+--------+\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|    PC 17599| 71.2833|  C85|       C|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|      113803|    53.1| C123|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|      113783|   26.55| C103|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|      248706|    16.0| NULL|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|      345763|    18.0| NULL|       S|\n",
            "|         26|       1|     3|Asplund, Mrs. Car...|female|38.0|    1|    5|      347077| 31.3875| NULL|       S|\n",
            "|         41|       0|     3|Ahlin, Mrs. Johan...|female|40.0|    1|    0|        7546|   9.475| NULL|       S|\n",
            "|         53|       1|     1|Harper, Mrs. Henr...|female|49.0|    1|    0|    PC 17572| 76.7292|  D33|       C|\n",
            "|         62|       1|     1| Icard, Miss. Amelie|female|38.0|    0|    0|      113572|    80.0|  B28|    NULL|\n",
            "|         86|       1|     3|Backstrom, Mrs. K...|female|33.0|    3|    0|     3101278|   15.85| NULL|       S|\n",
            "|         99|       1|     2|Doling, Mrs. John...|female|34.0|    0|    1|      231919|    23.0| NULL|       S|\n",
            "|        124|       1|     2| Webber, Miss. Susan|female|32.5|    0|    0|       27267|    13.0| E101|       S|\n",
            "|        133|       0|     3|Robins, Mrs. Alex...|female|47.0|    1|    0|   A/5. 3337|    14.5| NULL|       S|\n",
            "|        162|       1|     2|Watt, Mrs. James ...|female|40.0|    0|    0|  C.A. 33595|   15.75| NULL|       S|\n",
            "|        168|       0|     3|Skoog, Mrs. Willi...|female|45.0|    1|    4|      347088|    27.9| NULL|       S|\n",
            "|        178|       0|     1|Isham, Miss. Ann ...|female|50.0|    0|    0|    PC 17595| 28.7125|  C49|       C|\n",
            "|        191|       1|     2| Pinsky, Mrs. (Rosa)|female|32.0|    0|    0|      234604|    13.0| NULL|       S|\n",
            "|        195|       1|     1|Brown, Mrs. James...|female|44.0|    0|    0|    PC 17610| 27.7208|   B4|       C|\n",
            "|        196|       1|     1|Lurette, Miss. Elise|female|58.0|    0|    0|    PC 17569|146.5208|  B80|       C|\n",
            "|        212|       1|     2|Cameron, Miss. Cl...|female|35.0|    0|    0|F.C.C. 13528|    21.0| NULL|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+------------+--------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.where(\"Age > 30 and Sex == 'male'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX6IUTAW_0hn",
        "outputId": "bce4d6b6-1586-4656-dab3-b178c1886352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+----+----+-----+-----+-----------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name| Sex| Age|SibSp|Parch|     Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+----+----+-----+-----+-----------+-------+-----+--------+\n",
            "|          5|       0|     3|Allen, Mr. Willia...|male|35.0|    0|    0|     373450|   8.05| NULL|       S|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|male|54.0|    0|    0|      17463|51.8625|  E46|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|male|39.0|    1|    5|     347082| 31.275| NULL|       S|\n",
            "|         21|       0|     2|Fynney, Mr. Joseph J|male|35.0|    0|    0|     239865|   26.0| NULL|       S|\n",
            "|         22|       1|     2|Beesley, Mr. Lawr...|male|34.0|    0|    0|     248698|   13.0|  D56|       S|\n",
            "|         31|       0|     1|Uruchurtu, Don. M...|male|40.0|    0|    0|   PC 17601|27.7208| NULL|       C|\n",
            "|         34|       0|     2|Wheadon, Mr. Edwa...|male|66.0|    0|    0| C.A. 24579|   10.5| NULL|       S|\n",
            "|         36|       0|     1|Holverson, Mr. Al...|male|42.0|    1|    0|     113789|   52.0| NULL|       S|\n",
            "|         55|       0|     1|Ostby, Mr. Engelh...|male|65.0|    0|    1|     113509|61.9792|  B30|       C|\n",
            "|         63|       0|     1|Harris, Mr. Henry...|male|45.0|    1|    0|      36973| 83.475|  C83|       S|\n",
            "|         71|       0|     2|Jenkin, Mr. Steph...|male|32.0|    0|    0| C.A. 33111|   10.5| NULL|       S|\n",
            "|         75|       1|     3|       Bing, Mr. Lee|male|32.0|    0|    0|       1601|56.4958| NULL|       S|\n",
            "|         93|       0|     1|Chaffee, Mr. Herb...|male|46.0|    1|    0|W.E.P. 5734| 61.175|  E31|       S|\n",
            "|         95|       0|     3|   Coxon, Mr. Daniel|male|59.0|    0|    0|     364500|   7.25| NULL|       S|\n",
            "|         97|       0|     1|Goldschmidt, Mr. ...|male|71.0|    0|    0|   PC 17754|34.6542|   A5|       C|\n",
            "|        100|       0|     2|   Kantor, Mr. Sinai|male|34.0|    1|    0|     244367|   26.0| NULL|       S|\n",
            "|        104|       0|     3|Johansson, Mr. Gu...|male|33.0|    0|    0|       7540| 8.6542| NULL|       S|\n",
            "|        105|       0|     3|Gustafsson, Mr. A...|male|37.0|    2|    0|    3101276|  7.925| NULL|       S|\n",
            "|        109|       0|     3|     Rekic, Mr. Tido|male|38.0|    0|    0|     349249| 7.8958| NULL|       S|\n",
            "|        111|       0|     1|Porter, Mr. Walte...|male|47.0|    0|    0|     110465|   52.0| C110|       S|\n",
            "+-----------+--------+------+--------------------+----+----+-----+-----+-----------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.dropDuplicates().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSBVSQIqAJKG",
        "outputId": "bd8864e4-d4e7-4f5e-9b37-b7641a57fac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----------+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|    Fare|      Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----------+--------+\n",
            "|        680|       1|     1|Cardeza, Mr. Thom...|  male|36.0|    0|    1|PC 17755|512.3292|B51 B53 B55|       C|\n",
            "|        700|       0|     3|Humblen, Mr. Adol...|  male|42.0|    0|    0|  348121|    7.65|      F G63|       S|\n",
            "|        873|       0|     1|Carlsson, Mr. Fra...|  male|33.0|    0|    0|     695|     5.0|B51 B53 B55|       S|\n",
            "|        310|       1|     1|Francatelli, Miss...|female|30.0|    0|    0|PC 17485| 56.9292|        E36|       C|\n",
            "|        436|       1|     1|Carter, Miss. Luc...|female|14.0|    1|    2|  113760|   120.0|    B96 B98|       S|\n",
            "|        888|       1|     1|Graham, Miss. Mar...|female|19.0|    0|    0|  112053|    30.0|        B42|       S|\n",
            "|         76|       0|     3|Moen, Mr. Sigurd ...|  male|25.0|    0|    0|  348123|    7.65|      F G73|       S|\n",
            "|        810|       1|     1|Chambers, Mrs. No...|female|33.0|    1|    0|  113806|    53.1|         E8|       S|\n",
            "|        691|       1|     1|Dick, Mr. Albert ...|  male|31.0|    1|    0|   17474|    57.0|        B20|       S|\n",
            "|        138|       0|     1|Futrelle, Mr. Jac...|  male|37.0|    1|    0|  113803|    53.1|       C123|       S|\n",
            "|        210|       1|     1|    Blank, Mr. Henry|  male|40.0|    0|    0|  112277|    31.0|        A31|       C|\n",
            "|        610|       1|     1|Shutes, Miss. Eli...|female|40.0|    0|    0|PC 17582|153.4625|       C125|       S|\n",
            "|        766|       1|     1|Hogeboom, Mrs. Jo...|female|51.0|    1|    0|   13502| 77.9583|        D11|       S|\n",
            "|        880|       1|     1|Potter, Mrs. Thom...|female|56.0|    0|    1|   11767| 83.1583|        C50|       C|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|  113783|   26.55|       C103|       S|\n",
            "|        328|       1|     2|Ball, Mrs. (Ada E...|female|36.0|    0|    0|   28551|    13.0|          D|       S|\n",
            "|        578|       1|     1|Silvey, Mrs. Will...|female|39.0|    1|    0|   13507|    55.9|        E44|       S|\n",
            "|         22|       1|     2|Beesley, Mr. Lawr...|  male|34.0|    0|    0|  248698|    13.0|        D56|       S|\n",
            "|        119|       0|     1|Baxter, Mr. Quigg...|  male|24.0|    0|    1|PC 17558|247.5208|    B58 B60|       C|\n",
            "|        836|       1|     1|Compton, Miss. Sa...|female|39.0|    1|    1|PC 17756| 83.1583|        E49|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sorting"
      ],
      "metadata": {
        "id": "S12Jr_ujUjwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.sort(asc('PassengerId'),desc('Pclass')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVwNNXlCAaVc",
        "outputId": "c49f5733-104a-4546-c5d4-f42c37e644eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
            "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
            "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
            "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
            "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
            "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
            "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
            "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
            "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
            "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
            "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.agg({\"Age\": \"avg\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMNHJndUAtdX",
        "outputId": "d4bd69f1-350a-47b1-81db-8ce43851b042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|         avg(Age)|\n",
            "+-----------------+\n",
            "|29.69911764705882|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oHhSlNuA240",
        "outputId": "0dcbe5af-4fb6-4098-e0c6-ef7670011e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic = titanic.repartition(5)"
      ],
      "metadata": {
        "id": "gNeTSy07BnR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-fRRZerCCsg",
        "outputId": "3c99a87d-c599-4f11-9c99-063b45f2ec6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"John\", \"Sales\", 3000, \"2023-01-10\", \"New York\"),\n",
        "    (2, \"Mike\", \"Sales\", 4600, \"2022-03-15\", \"New York\"),\n",
        "    (3, \"Sara\", \"HR\", 4100, \"2021-08-20\", \"Texas\"),\n",
        "    (4, \"Kate\", \"Finance\", 5400, \"2022-06-10\", \"California\"),\n",
        "    (5, \"David\", \"Finance\", 3300, \"2023-05-25\", \"California\"),\n",
        "    (6, \"Chris\", \"HR\", 4000, \"2021-12-12\", \"Texas\"),\n",
        "    (7, \"Emma\", \"Sales\", 5200, \"2023-04-10\", \"New York\"),\n",
        "    (8, \"Robert\", \"IT\", 6100, \"2020-11-05\", \"Texas\"),\n",
        "    (9, \"Sophia\", \"IT\", 6800, \"2022-01-25\", \"Texas\"),\n",
        "    (10, \"James\", \"Finance\", 3900, \"2023-06-14\", \"California\")\n",
        "]\n",
        "\n",
        "columns = [\"emp_id\", \"name\", \"department\", \"salary\", \"joining_date\", \"city\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "Z-0qkr17Jmkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81708d06-ceb8-458d-c863-edb992c9e664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+------------+----------+\n",
            "|emp_id|  name|department|salary|joining_date|      city|\n",
            "+------+------+----------+------+------------+----------+\n",
            "|     1|  John|     Sales|  3000|  2023-01-10|  New York|\n",
            "|     2|  Mike|     Sales|  4600|  2022-03-15|  New York|\n",
            "|     3|  Sara|        HR|  4100|  2021-08-20|     Texas|\n",
            "|     4|  Kate|   Finance|  5400|  2022-06-10|California|\n",
            "|     5| David|   Finance|  3300|  2023-05-25|California|\n",
            "|     6| Chris|        HR|  4000|  2021-12-12|     Texas|\n",
            "|     7|  Emma|     Sales|  5200|  2023-04-10|  New York|\n",
            "|     8|Robert|        IT|  6100|  2020-11-05|     Texas|\n",
            "|     9|Sophia|        IT|  6800|  2022-01-25|     Texas|\n",
            "|    10| James|   Finance|  3900|  2023-06-14|California|\n",
            "+------+------+----------+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the schema and count the total number of rows."
      ],
      "metadata": {
        "id": "nuUdo3G0VFtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll-aFbDmVBSh",
        "outputId": "8bc9976f-ef3f-4ced-8348-0b959691cc42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- joining_date: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPZHTeU_VICw",
        "outputId": "d2ca07cb-4105-47cd-d4a9-6fd398565e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display only 5 records."
      ],
      "metadata": {
        "id": "vadZ2iKbVQs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onu1kGUYVOEQ",
        "outputId": "2849b54e-307f-40d4-8214-7bd41747cb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----------+------+------------+----------+\n",
            "|emp_id| name|department|salary|joining_date|      city|\n",
            "+------+-----+----------+------+------------+----------+\n",
            "|     1| John|     Sales|  3000|  2023-01-10|  New York|\n",
            "|     2| Mike|     Sales|  4600|  2022-03-15|  New York|\n",
            "|     3| Sara|        HR|  4100|  2021-08-20|     Texas|\n",
            "|     4| Kate|   Finance|  5400|  2022-06-10|California|\n",
            "|     5|David|   Finance|  3300|  2023-05-25|California|\n",
            "+------+-----+----------+------+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename column joining_date to join_date."
      ],
      "metadata": {
        "id": "XVqErUvTVUnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumnRenamed('joining_date','join_date')"
      ],
      "metadata": {
        "id": "-6qNzTWaVSEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select only name, department, and salary."
      ],
      "metadata": {
        "id": "c7iI2gHdVfUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('name','department','salary').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EilwgObjVbWS",
        "outputId": "51f5d485-64ff-4693-9d7d-24cdddad3619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| name|department|salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Mike|     Sales|  4600|\n",
            "| Sara|        HR|  4100|\n",
            "| Kate|   Finance|  5400|\n",
            "|David|   Finance|  3300|\n",
            "+-----+----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter employees whose salary is greater than 4000."
      ],
      "metadata": {
        "id": "eWVbwKsfVo-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.salary>4000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdleGXhZVl-U",
        "outputId": "acfc44e2-087d-4567-94f7-5641c507a513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+----------+----------+\n",
            "|emp_id|  name|department|salary| join_date|      city|\n",
            "+------+------+----------+------+----------+----------+\n",
            "|     2|  Mike|     Sales|  4600|2022-03-15|  New York|\n",
            "|     3|  Sara|        HR|  4100|2021-08-20|     Texas|\n",
            "|     4|  Kate|   Finance|  5400|2022-06-10|California|\n",
            "|     7|  Emma|     Sales|  5200|2023-04-10|  New York|\n",
            "|     8|Robert|        IT|  6100|2020-11-05|     Texas|\n",
            "|     9|Sophia|        IT|  6800|2022-01-25|     Texas|\n",
            "+------+------+----------+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show all distinct departments."
      ],
      "metadata": {
        "id": "Go4TE0FwV2Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('department').distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1gnPzXNVz_d",
        "outputId": "4a8f9b4b-36f5-40c0-b51b-47ea2c129e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|department|\n",
            "+----------+\n",
            "|     Sales|\n",
            "|        HR|\n",
            "|   Finance|\n",
            "|        IT|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count how many employees are in each department."
      ],
      "metadata": {
        "id": "gqukL-QtV891"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "7H_Fp5AkXdWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('department').count().show()\n",
        "df.groupBy('department').max('salary').show()\n",
        "df.groupBy('department').avg('salary').show()\n",
        "df.groupBy('department').agg(max('salary').alias('max_salary'),min('salary').alias('min_salary')).show()\n",
        "df.groupBy(['department','city']).max('salary').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMfb6ueKV6vA",
        "outputId": "9757ac13-8890-4d25-d9a5-84436083f3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|department|count|\n",
            "+----------+-----+\n",
            "|     Sales|    3|\n",
            "|        HR|    2|\n",
            "|   Finance|    3|\n",
            "|        IT|    2|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----------+\n",
            "|department|max(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       5200|\n",
            "|        HR|       4100|\n",
            "|   Finance|       5400|\n",
            "|        IT|       6800|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------------+\n",
            "|department|      avg(salary)|\n",
            "+----------+-----------------+\n",
            "|     Sales|4266.666666666667|\n",
            "|        HR|           4050.0|\n",
            "|   Finance|           4200.0|\n",
            "|        IT|           6450.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|department|max_salary|min_salary|\n",
            "+----------+----------+----------+\n",
            "|     Sales|      5200|      3000|\n",
            "|        HR|      4100|      4000|\n",
            "|   Finance|      5400|      3300|\n",
            "|        IT|      6800|      6100|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+-----------+\n",
            "|department|      city|max(salary)|\n",
            "+----------+----------+-----------+\n",
            "|   Finance|California|       5400|\n",
            "|        HR|     Texas|       4100|\n",
            "|     Sales|  New York|       5200|\n",
            "|        IT|     Texas|       6800|\n",
            "+----------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort employees by salary descending."
      ],
      "metadata": {
        "id": "lnUn8kRIX86y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(desc('salary'),asc('emp_id')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux36yKT2WC2K",
        "outputId": "accdf669-2ba8-4d22-c4a6-3ea74d9f0ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+----------+----------+\n",
            "|emp_id|  name|department|salary| join_date|      city|\n",
            "+------+------+----------+------+----------+----------+\n",
            "|     9|Sophia|        IT|  6800|2022-01-25|     Texas|\n",
            "|     8|Robert|        IT|  6100|2020-11-05|     Texas|\n",
            "|     4|  Kate|   Finance|  5400|2022-06-10|California|\n",
            "|     7|  Emma|     Sales|  5200|2023-04-10|  New York|\n",
            "|     2|  Mike|     Sales|  4600|2022-03-15|  New York|\n",
            "|     3|  Sara|        HR|  4100|2021-08-20|     Texas|\n",
            "|     6| Chris|        HR|  4000|2021-12-12|     Texas|\n",
            "|    10| James|   Finance|  3900|2023-06-14|California|\n",
            "|     5| David|   Finance|  3300|2023-05-25|California|\n",
            "|     1|  John|     Sales|  3000|2023-01-10|  New York|\n",
            "+------+------+----------+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a new column bonus = 10% of salary."
      ],
      "metadata": {
        "id": "F7IxUMe-YPaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"Bonus\",col('salary')*0.1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gKo67JfX-oJ",
        "outputId": "3f9a4d13-6cbc-41bb-856b-bfc9ca238e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+----------+----------+-----+\n",
            "|emp_id|  name|department|salary| join_date|      city|Bonus|\n",
            "+------+------+----------+------+----------+----------+-----+\n",
            "|     1|  John|     Sales|  3000|2023-01-10|  New York|300.0|\n",
            "|     2|  Mike|     Sales|  4600|2022-03-15|  New York|460.0|\n",
            "|     3|  Sara|        HR|  4100|2021-08-20|     Texas|410.0|\n",
            "|     4|  Kate|   Finance|  5400|2022-06-10|California|540.0|\n",
            "|     5| David|   Finance|  3300|2023-05-25|California|330.0|\n",
            "|     6| Chris|        HR|  4000|2021-12-12|     Texas|400.0|\n",
            "|     7|  Emma|     Sales|  5200|2023-04-10|  New York|520.0|\n",
            "|     8|Robert|        IT|  6100|2020-11-05|     Texas|610.0|\n",
            "|     9|Sophia|        IT|  6800|2022-01-25|     Texas|680.0|\n",
            "|    10| James|   Finance|  3900|2023-06-14|California|390.0|\n",
            "+------+------+----------+------+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('city','name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15UG8I7KYZq-",
        "outputId": "b82c1599-68a5-4295-cb57-f4233da5dc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+------+----------+\n",
            "|emp_id|department|salary| join_date|\n",
            "+------+----------+------+----------+\n",
            "|     1|     Sales|  3000|2023-01-10|\n",
            "|     2|     Sales|  4600|2022-03-15|\n",
            "|     3|        HR|  4100|2021-08-20|\n",
            "|     4|   Finance|  5400|2022-06-10|\n",
            "|     5|   Finance|  3300|2023-05-25|\n",
            "|     6|        HR|  4000|2021-12-12|\n",
            "|     7|     Sales|  5200|2023-04-10|\n",
            "|     8|        IT|  6100|2020-11-05|\n",
            "|     9|        IT|  6800|2022-01-25|\n",
            "|    10|   Finance|  3900|2023-06-14|\n",
            "+------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the average salary per department."
      ],
      "metadata": {
        "id": "3QMpUsfPYsrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('department').avg('salary').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlwExDXOYh7J",
        "outputId": "e7a34414-e9ec-4c0a-d53e-451efbf24128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|department|      avg(salary)|\n",
            "+----------+-----------------+\n",
            "|     Sales|4266.666666666667|\n",
            "|        HR|           4050.0|\n",
            "|   Finance|           4200.0|\n",
            "|        IT|           6450.0|\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the maximum and minimum salary in each city."
      ],
      "metadata": {
        "id": "P8vhEfZnY3-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('city').agg(max('salary'),min('salary')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riv-qQEvY1Mx",
        "outputId": "bc98aaae-255d-451e-d823-f4da57171b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------+\n",
            "|      city|max(salary)|min(salary)|\n",
            "+----------+-----------+-----------+\n",
            "|     Texas|       6800|       4000|\n",
            "|California|       5400|       3300|\n",
            "|  New York|       5200|       3000|\n",
            "+----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(sum('salary')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Rc7vuRY96X",
        "outputId": "7b3d902b-b88e-413d-f54a-1f6b8a1d4371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|sum(salary)|\n",
            "+-----------+\n",
            "|      46400|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the department that has the highest average salary."
      ],
      "metadata": {
        "id": "r3RFFsL8abNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = df.groupBy('department').avg('salary')"
      ],
      "metadata": {
        "id": "2QozFNrIZEsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = d.withColumnRenamed(\"avg(salary)\",'average')"
      ],
      "metadata": {
        "id": "8NtUix1OZv8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.sort(desc('average')).limit(1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp6zMkRBZato",
        "outputId": "0be16ce2-e300-4a4f-aaa2-d9d720f99007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n",
            "|department|average|\n",
            "+----------+-------+\n",
            "|        IT| 6450.0|\n",
            "+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a column salary_level — label it as “Low” (<4000), “Medium” (4000–6000), “High” (>6000)."
      ],
      "metadata": {
        "id": "ExHYucVfadPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('salary_level',when(col('salary')<4000,'Low').\n",
        "              when((col(\"salary\") >= 4000) & (col(\"salary\") < 6000), \"Medium\").\n",
        "              otherwise('High')\n",
        "              ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWdCNxXeZmoM",
        "outputId": "9d4aaf27-690f-4b72-a44b-088639680c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+----------+----------+------------+\n",
            "|emp_id|  name|department|salary| join_date|      city|salary_level|\n",
            "+------+------+----------+------+----------+----------+------------+\n",
            "|     1|  John|     Sales|  3000|2023-01-10|  New York|         Low|\n",
            "|     2|  Mike|     Sales|  4600|2022-03-15|  New York|      Medium|\n",
            "|     3|  Sara|        HR|  4100|2021-08-20|     Texas|      Medium|\n",
            "|     4|  Kate|   Finance|  5400|2022-06-10|California|      Medium|\n",
            "|     5| David|   Finance|  3300|2023-05-25|California|         Low|\n",
            "|     6| Chris|        HR|  4000|2021-12-12|     Texas|      Medium|\n",
            "|     7|  Emma|     Sales|  5200|2023-04-10|  New York|      Medium|\n",
            "|     8|Robert|        IT|  6100|2020-11-05|     Texas|        High|\n",
            "|     9|Sophia|        IT|  6800|2022-01-25|     Texas|        High|\n",
            "|    10| James|   Finance|  3900|2023-06-14|California|         Low|\n",
            "+------+------+----------+------+----------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out employees who joined after “2022-12-31”."
      ],
      "metadata": {
        "id": "fIlll3fIctpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AQ6UrzfbPpd",
        "outputId": "81309944-64a4-4e64-9481-3542fa22b175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- join_date: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('join_date',df.join_date.cast('date'))"
      ],
      "metadata": {
        "id": "1CUPyHficvUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col('join_date')>'2022-12-31').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eqJtoH6dEaV",
        "outputId": "3200e733-2279-4b44-b8d8-57c244dd2a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----------+------+----------+----------+\n",
            "|emp_id| name|department|salary| join_date|      city|\n",
            "+------+-----+----------+------+----------+----------+\n",
            "|     1| John|     Sales|  3000|2023-01-10|  New York|\n",
            "|     5|David|   Finance|  3300|2023-05-25|California|\n",
            "|     7| Emma|     Sales|  5200|2023-04-10|  New York|\n",
            "|    10|James|   Finance|  3900|2023-06-14|California|\n",
            "+------+-----+----------+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the total number of employees in each city and sort descending."
      ],
      "metadata": {
        "id": "XZThg81qdqdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('city').count().sort(desc('count')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnItavDUdTkI",
        "outputId": "bf6055d4-fea3-4821-966e-944b10b4fa26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|      city|count|\n",
            "+----------+-----+\n",
            "|     Texas|    4|\n",
            "|California|    3|\n",
            "|  New York|    3|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the highest-paid employee per department."
      ],
      "metadata": {
        "id": "tZbi9kWyd9fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(col('department'))['name'].max('salary').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "UQREifONdv-P",
        "outputId": "51b93694-a4f7-4b2f-a9f5-fae58c14655d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'GroupedData' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1798550811.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'department'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'salary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'GroupedData' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDjfYt33eHLb",
        "outputId": "027ec7ee-2a58-472e-c6a3-536aaa9402af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+----------+----------+\n",
            "|emp_id|  name|department|salary| join_date|      city|\n",
            "+------+------+----------+------+----------+----------+\n",
            "|     1|  John|     Sales|  3000|2023-01-10|  New York|\n",
            "|     2|  Mike|     Sales|  4600|2022-03-15|  New York|\n",
            "|     3|  Sara|        HR|  4100|2021-08-20|     Texas|\n",
            "|     4|  Kate|   Finance|  5400|2022-06-10|California|\n",
            "|     5| David|   Finance|  3300|2023-05-25|California|\n",
            "|     6| Chris|        HR|  4000|2021-12-12|     Texas|\n",
            "|     7|  Emma|     Sales|  5200|2023-04-10|  New York|\n",
            "|     8|Robert|        IT|  6100|2020-11-05|     Texas|\n",
            "|     9|Sophia|        IT|  6800|2022-01-25|     Texas|\n",
            "|    10| James|   Finance|  3900|2023-06-14|California|\n",
            "+------+------+----------+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "6TqGjmC-sWTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_func =Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())"
      ],
      "metadata": {
        "id": "1U-uGJggsf5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"row_number\",row_number().over(window_func)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ol_d5PxshjE",
        "outputId": "61f8d4a1-a338-4d85-b30b-b1600d4377bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+----------+------+----------+----------+----------+\n",
            "|emp_id|  name|department|salary| join_date|      city|row_number|\n",
            "+------+------+----------+------+----------+----------+----------+\n",
            "|     4|  Kate|   Finance|  5400|2022-06-10|California|         1|\n",
            "|    10| James|   Finance|  3900|2023-06-14|California|         2|\n",
            "|     5| David|   Finance|  3300|2023-05-25|California|         3|\n",
            "|     3|  Sara|        HR|  4100|2021-08-20|     Texas|         1|\n",
            "|     6| Chris|        HR|  4000|2021-12-12|     Texas|         2|\n",
            "|     9|Sophia|        IT|  6800|2022-01-25|     Texas|         1|\n",
            "|     8|Robert|        IT|  6100|2020-11-05|     Texas|         2|\n",
            "|     7|  Emma|     Sales|  5200|2023-04-10|  New York|         1|\n",
            "|     2|  Mike|     Sales|  4600|2022-03-15|  New York|         2|\n",
            "|     1|  John|     Sales|  3000|2023-01-10|  New York|         3|\n",
            "+------+------+----------+------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, sum, row_number\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (\"A\", 10),\n",
        "    (\"A\", 20),\n",
        "    (\"A\", 30),\n",
        "    (\"B\", 5),\n",
        "    (\"B\", 15),\n",
        "], [\"category\", \"value\"])\n"
      ],
      "metadata": {
        "id": "GMlr0IOvszrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.partitionBy(\"category\").orderBy(\"value\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
        "df.withColumn(\"running_total\", sum(\"value\").over(w)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ6EqdI3glkW",
        "outputId": "18c85345-fc40-4ba6-933a-3fe3a01c499b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-------------+\n",
            "|category|value|running_total|\n",
            "+--------+-----+-------------+\n",
            "|       A|   10|           10|\n",
            "|       A|   20|           30|\n",
            "|       A|   30|           60|\n",
            "|       B|    5|            5|\n",
            "|       B|   15|           20|\n",
            "+--------+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.partitionBy(\"category\").orderBy(\"value\") \\\n",
        "         .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
        "\n",
        "df.withColumn(\"future_sum\", sum(\"value\").over(w)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzjFa7XFg0g_",
        "outputId": "6a625c57-3ca0-42ee-cc19-31998181bb8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+----------+\n",
            "|category|value|future_sum|\n",
            "+--------+-----+----------+\n",
            "|       A|   10|        60|\n",
            "|       A|   20|        50|\n",
            "|       A|   30|        30|\n",
            "|       B|    5|        20|\n",
            "|       B|   15|        15|\n",
            "+--------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.partitionBy(\"category\").orderBy(\"value\")\n",
        "\n",
        "df.withColumn(\"range_sum\", sum(\"value\").over(w)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQnhB0Kg9zc",
        "outputId": "863e4ca5-ff7e-4f94-9da6-fb27750b18a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+---------+\n",
            "|category|value|range_sum|\n",
            "+--------+-----+---------+\n",
            "|       A|   10|       10|\n",
            "|       A|   20|       30|\n",
            "|       A|   30|       60|\n",
            "|       B|    5|        5|\n",
            "|       B|   15|       20|\n",
            "+--------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice DUPLICATE RECORDS OF ALL COLUMNS"
      ],
      "metadata": {
        "id": "yHb5cMcH3C5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HIRED WITH IN LAST N DAYS ,N MONTHS, N YEARS\n",
        "DATE_ADD(DATE,1/-1) FOR DAYS\n",
        "ADD_MONTHS(DATE,1/-1) FOR MONTHS AND YEARS"
      ],
      "metadata": {
        "id": "fYzPZ3l627BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName('Nik').getOrCreate()\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "data = [\n",
        "    (1,  \"Alice\",   \"IT\",       date(2025, 1, 5)),\n",
        "    (2,  \"Bob\",     \"IT\",       date(2024, 12, 20)),\n",
        "    (3,  \"Charlie\", \"HR\",       date(2024, 11, 15)),\n",
        "    (4,  \"David\",   \"Finance\",  date(2024, 9, 10)),\n",
        "    (5,  \"Eva\",     \"Finance\",  date(2023, 12, 1)),\n",
        "    (6,  \"Frank\",   \"HR\",       date(2023, 5, 18)),\n",
        "    (7,  \"George\",  \"IT\",       date(2022, 8, 25)),\n",
        "    (8,  \"Helen\",   \"Finance\",  date(2021, 3, 5)),\n",
        "    (9,  \"Ian\",     \"IT\",       date(2020, 12, 15)),\n",
        "    (10, \"Jane\",    \"HR\",       date(2019, 7, 30))\n",
        "]\n",
        "columns = [\"emp_id\", \"name\", \"dept\", \"hire_date\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "df.createOrReplaceTempView('Employee')\n",
        "df.filter(col('hire_date')>=add_months(curdate(),-3*12)).show()\n",
        "spark.sql(\"select * from Employee where hire_date >= date_add(current_date,-2*12) \").show()\n",
        "spark.sql(\"select * from Employee where hire_date >= add_months(current_date,-135) \").show()"
      ],
      "metadata": {
        "id": "II2hR1vBhDbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744783c7-e4f5-4bf5-b071-dc7a001640b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-------+----------+\n",
            "|emp_id|   name|   dept| hire_date|\n",
            "+------+-------+-------+----------+\n",
            "|     1|  Alice|     IT|2025-01-05|\n",
            "|     2|    Bob|     IT|2024-12-20|\n",
            "|     3|Charlie|     HR|2024-11-15|\n",
            "|     4|  David|Finance|2024-09-10|\n",
            "|     5|    Eva|Finance|2023-12-01|\n",
            "|     6|  Frank|     HR|2023-05-18|\n",
            "|     7| George|     IT|2022-08-25|\n",
            "|     8|  Helen|Finance|2021-03-05|\n",
            "|     9|    Ian|     IT|2020-12-15|\n",
            "|    10|   Jane|     HR|2019-07-30|\n",
            "+------+-------+-------+----------+\n",
            "\n",
            "+------+-------+-------+----------+\n",
            "|emp_id|   name|   dept| hire_date|\n",
            "+------+-------+-------+----------+\n",
            "|     1|  Alice|     IT|2025-01-05|\n",
            "|     2|    Bob|     IT|2024-12-20|\n",
            "|     3|Charlie|     HR|2024-11-15|\n",
            "|     4|  David|Finance|2024-09-10|\n",
            "|     5|    Eva|Finance|2023-12-01|\n",
            "|     6|  Frank|     HR|2023-05-18|\n",
            "+------+-------+-------+----------+\n",
            "\n",
            "+------+----+----+---------+\n",
            "|emp_id|name|dept|hire_date|\n",
            "+------+----+----+---------+\n",
            "+------+----+----+---------+\n",
            "\n",
            "+------+-------+-------+----------+\n",
            "|emp_id|   name|   dept| hire_date|\n",
            "+------+-------+-------+----------+\n",
            "|     1|  Alice|     IT|2025-01-05|\n",
            "|     2|    Bob|     IT|2024-12-20|\n",
            "|     3|Charlie|     HR|2024-11-15|\n",
            "|     4|  David|Finance|2024-09-10|\n",
            "|     5|    Eva|Finance|2023-12-01|\n",
            "|     6|  Frank|     HR|2023-05-18|\n",
            "|     7| George|     IT|2022-08-25|\n",
            "|     8|  Helen|Finance|2021-03-05|\n",
            "|     9|    Ian|     IT|2020-12-15|\n",
            "|    10|   Jane|     HR|2019-07-30|\n",
            "+------+-------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Given a cricket match dataset with columns Team_1, Team_2, and Winner, write a Spark SQL query and an equivalent PySpark transformation to calculate for each team: total matches played, total wins, total losses. A team gets 1 win if it appears in the Winner column, and every team plays exactly one match in each row."
      ],
      "metadata": {
        "id": "iGHrQDQyi2gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Team_1\": [\"India\", \"SL\", \"SA\", \"Eng\", \"Aus\"],\n",
        "    \"Team_2\": [\"SL\", \"Aus\", \"Eng\", \"NZ\", \"India\"],\n",
        "    \"Winner\": [\"India\", \"Aus\", \"Eng\", \"NZ\", \"India\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df = spark.createDataFrame(df)\n",
        "df.createOrReplaceTempView('ipl')\n",
        "#SQL\n",
        "spark.sql(\"\"\"with cte as (select Team_1,CASE WHEN Team_1=Winner THEN 1 ELSE 0 END as flag from ipl\n",
        "            UNION ALL\n",
        "            select Team_2,CASE WHEN Team_2=Winner THEN 1 ELSE 0 END as flag from ipl)\n",
        "            select Team_1,count(Team_1) as matches_played,sum(flag) as win,2-win as loss from cte  group by Team_1\n",
        "            \"\"\").show()\n",
        "#PYSPARK\n",
        "final_df = df.select(col('Team_1'),when(col('Team_1')==col('Winner'),1).otherwise(0).alias('win')).union(df.select(col('Team_2'),when(col('Team_2')==col('Winner'),1).otherwise(0).alias('win')))\n",
        "df = final_df.groupBy('Team_1').agg(sum(col('win')).alias('win'),count('Team_1').alias('played'))\n",
        "df = df.withColumnRenamed('Team_1','Teams')\n",
        "df.select('Teams','win','played',lit(2-col('win')).alias('loss')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAYGf2-MhRdd",
        "outputId": "d8665abc-2d58-4228-bff6-b1b213c7d6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+---+----+\n",
            "|Team_1|matches_played|win|loss|\n",
            "+------+--------------+---+----+\n",
            "|    SL|             2|  0|   2|\n",
            "| India|             2|  2|   0|\n",
            "|   Eng|             2|  1|   1|\n",
            "|    SA|             1|  0|   2|\n",
            "|   Aus|             2|  1|   1|\n",
            "|    NZ|             1|  1|   1|\n",
            "+------+--------------+---+----+\n",
            "\n",
            "+-----+---+------+----+\n",
            "|Teams|win|played|loss|\n",
            "+-----+---+------+----+\n",
            "|   SL|  0|     2|   2|\n",
            "|India|  2|     2|   0|\n",
            "|  Eng|  1|     2|   1|\n",
            "|   SA|  0|     1|   2|\n",
            "|  Aus|  1|     2|   1|\n",
            "|   NZ|  1|     1|   1|\n",
            "+-----+---+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the given orders dataset, write a Spark SQL query to find how many customers are new and how many are returning on each order date. A customer is considered new if the order date is their first visit date, otherwise they are considered returning. Group the results by order_date and display the counts in ascending order."
      ],
      "metadata": {
        "id": "1tuERJRd2gGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"order_id\": [1,2,3,4,5,6,7,8,9],\n",
        "    \"customer_id\": [100,200,300,100,400,500,100,400,600],\n",
        "    \"order_date\": [\n",
        "        \"2022-01-01\",\"2022-01-01\",\"2022-01-01\",\n",
        "        \"2022-01-02\",\"2022-01-02\",\"2022-01-02\",\n",
        "        \"2022-01-03\",\"2022-01-03\",\"2022-01-03\"\n",
        "    ],\n",
        "    \"order_amount\": [2000,2500,2100,2000,2200,2700,3000,3000,3000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['order_date'] = pd.to_datetime(df['order_date']).dt.date\n",
        "print(df)\n",
        "sdf = spark.createDataFrame(df)\n",
        "sdf.createOrReplaceTempView('customers')\n",
        "spark.sql(\"\"\"with fv as (SELECT customer_id,min(order_date) as first_visit_date from customers group by customer_id)\n",
        "            select order_date,sum(new_customers),sum(repeated_customers) from\n",
        "            (select order_date,case when order_date=first_visit_date then 1 else 0 end as new_customers ,\n",
        "            case when order_date!=first_visit_date then 1 else 0 end as repeated_customers\n",
        "            from customers c LEFT JOIN fv on c.customer_id = fv.customer_id)\n",
        "            group by order_date\n",
        "            order by order_date asc;\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kZ2cPi0wvfO",
        "outputId": "4a64dce4-4531-4c5f-c252-1bad069fb875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   order_id  customer_id  order_date  order_amount\n",
            "0         1          100  2022-01-01          2000\n",
            "1         2          200  2022-01-01          2500\n",
            "2         3          300  2022-01-01          2100\n",
            "3         4          100  2022-01-02          2000\n",
            "4         5          400  2022-01-02          2200\n",
            "5         6          500  2022-01-02          2700\n",
            "6         7          100  2022-01-03          3000\n",
            "7         8          400  2022-01-03          3000\n",
            "8         9          600  2022-01-03          3000\n",
            "+----------+------------------+-----------------------+\n",
            "|order_date|sum(new_customers)|sum(repeated_customers)|\n",
            "+----------+------------------+-----------------------+\n",
            "|2022-01-01|                 3|                      0|\n",
            "|2022-01-02|                 2|                      1|\n",
            "|2022-01-03|                 1|                      2|\n",
            "+----------+------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5pRdzh5xjD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}